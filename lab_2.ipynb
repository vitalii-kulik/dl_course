{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ad372e4a-1af3-4b77-b71c-2c50572d7692",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9d52be67-6080-49bb-b689-c05a993205b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameter:\n",
    "    def __init__(self, value: float, name: str, _prev=()) -> None:\n",
    "        self._value = value\n",
    "        self._name = name\n",
    "        self._grad = 0.0\n",
    "        self._backward = lambda: None\n",
    "        self._prev = set(_prev)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Parameter {self._name} = {self._value}; dL/d[{self._name}] = {self._grad}\"\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self._grad = 0.0\n",
    "\n",
    "    def __mul__(self, other: 'Parameter') -> 'Parameter':\n",
    "        result = Parameter(\n",
    "            self._value * other._value,\n",
    "            f'({self._name} * {other._name})',\n",
    "            (self, other)\n",
    "        )\n",
    "        \n",
    "        def _backward():\n",
    "            self._grad += other._value * result._grad\n",
    "            other._grad += self._value * result._grad\n",
    "\n",
    "        result._backward = _backward\n",
    "        return result\n",
    "\n",
    "    def __add__(self, other: 'Parameter') -> 'Parameter':\n",
    "        result = Parameter(\n",
    "            self._value + other._value,\n",
    "            f'({self._name} + {other._name})',\n",
    "            (self, other)\n",
    "        )\n",
    "        \n",
    "        def _backward():\n",
    "            self._grad += 1.0 * result._grad\n",
    "            other._grad += 1.0 * result._grad\n",
    "\n",
    "        result._backward = _backward\n",
    "        return result\n",
    "\n",
    "    def relu(self) -> 'Parameter':\n",
    "        result_value = max(0.0, self._value)\n",
    "        result = Parameter(\n",
    "            result_value,\n",
    "            f'relu({self._name})',\n",
    "            (self,)\n",
    "        )\n",
    "        \n",
    "        def _backward():\n",
    "            self._grad += (1.0 if self._value > 0 else 0.0) * result._grad\n",
    "\n",
    "        result._backward = _backward\n",
    "        return result\n",
    "\n",
    "    def sigmoid(self) -> 'Parameter':\n",
    "        sig_value = 1 / (1 + math.exp(-self._value))\n",
    "        result = Parameter(\n",
    "            sig_value,\n",
    "            f'sigmoid({self._name})',\n",
    "            (self,)\n",
    "        )\n",
    "        \n",
    "        def _backward():\n",
    "            self._grad += sig_value * (1 - sig_value) * result._grad\n",
    "\n",
    "        result._backward = _backward\n",
    "        return result\n",
    "\n",
    "    def silu(self) -> 'Parameter':\n",
    "        sigmoid = self.sigmoid()\n",
    "        result = self * sigmoid\n",
    "        result._name = f'silu({self._name})'\n",
    "        \n",
    "        def _backward():\n",
    "            self._grad += (sigmoid._value + self._value * sigmoid._value * (1 - sigmoid._value)) * result._grad\n",
    "            # Note: sigmoid's backward will be called when self.sigmoid() is used, no need to update sigmoid._grad here\n",
    "\n",
    "        result._backward = _backward\n",
    "        return result\n",
    "\n",
    "    def backward(self):\n",
    "        # comp graph in topological order\n",
    "        topo = []\n",
    "        visited = set()\n",
    "\n",
    "        def build_topo(param):\n",
    "            if param not in visited:\n",
    "                visited.add(param)\n",
    "                for child in param._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(param)\n",
    "        \n",
    "        build_topo(self)\n",
    "\n",
    "        # reset gradients\n",
    "        for param in topo:\n",
    "            param._grad = 0.0\n",
    "\n",
    "        # set the gradient of the output\n",
    "        self._grad = 1.0\n",
    "\n",
    "        # propagate gradients\n",
    "        for param in reversed(topo):\n",
    "            param._backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7df6fa71-7962-4b8e-99f5-0ad52f56b57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 46.84976766430972\n",
      "Epoch 2, Loss: 4.762173652918586\n",
      "Epoch 3, Loss: 3.00929726116663\n",
      "Epoch 4, Loss: 1.9016253219818784\n",
      "Epoch 5, Loss: 1.2016688786008447\n",
      "Epoch 6, Loss: 0.7593546831259383\n",
      "Epoch 7, Loss: 0.47984893763469993\n",
      "Epoch 8, Loss: 0.30322457748108844\n",
      "Epoch 9, Loss: 0.191612687196531\n",
      "Epoch 10, Loss: 0.12108326508251104\n",
      "Epoch 11, Loss: 0.07651454242173733\n",
      "Epoch 12, Loss: 0.04835082038808792\n",
      "Epoch 13, Loss: 0.03055369290866978\n",
      "Epoch 14, Loss: 0.01930739008902733\n",
      "Epoch 15, Loss: 0.012200663047970548\n",
      "Epoch 16, Loss: 0.007709803247551579\n",
      "Epoch 17, Loss: 0.004871953752205555\n",
      "Epoch 18, Loss: 0.0030786691438805364\n",
      "Epoch 19, Loss: 0.0019454625761160394\n",
      "Epoch 20, Loss: 0.0012293703734259876\n",
      "Learned w: 3.002916668942686, b: 1.9690519687344459\n"
     ]
    }
   ],
   "source": [
    "def sgd(parameters: list[Parameter], lr: float) -> None:\n",
    "    for param in parameters:\n",
    "        param._value -= lr * param._grad\n",
    "\n",
    "def mse_loss(y_pred: Parameter, y_true: float) -> Parameter:\n",
    "    diff = y_pred + Parameter(-y_true, '-y_true')\n",
    "    return diff * diff\n",
    "\n",
    "def train(model: LinearModel, x_train: list[float], y_train: list[float], lr: float = 0.1, epochs: int = 100):\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for x_val, y_val in zip(x_train, y_train):\n",
    "            x_param = Parameter(x_val, 'x')\n",
    "            y_pred = model(x_param)\n",
    "\n",
    "            loss = mse_loss(y_pred, y_val)\n",
    "\n",
    "            # backpropagate\n",
    "            loss.backward()\n",
    "\n",
    "            # update parameters\n",
    "            sgd([model.w, model.b], lr)\n",
    "\n",
    "            total_loss += loss._value\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(x_train)}\")\n",
    "\n",
    "# training data for the model y = 3x + 2\n",
    "x_train = [i for i in range(-10, 11)]\n",
    "y_train = [3 * x + 2 for x in x_train]\n",
    "\n",
    "# initialize\n",
    "model = LinearModel()\n",
    "\n",
    "# train\n",
    "train(model, x_train, y_train, lr=0.01, epochs=20)\n",
    "\n",
    "# validation\n",
    "print(f\"Learned w: {model.w._value}, b: {model.b._value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
